# Voice AI Setup & Testing Guide

## Quick Start (3 Steps)

### Step 1: Start the API Server
```bash
# Terminal 1: In project root
cd D:\Projects\Project-Nalam
.\.venv\Scripts\activate
python api_server.py
```

Expected output:
```
 * Running on http://127.0.0.1:5000
Press CTRL+C to quit
```

### Step 2: Start the Expo App
```bash
# Terminal 2: In client folder
cd D:\Projects\Project-Nalam\client
npm start
```

Then press:
- **i** to open in iOS simulator (if on Mac)
- **a** to open in Android emulator (if Windows with Android Studio)
- **w** to open in web (for testing, but voice won't work in browser)

### Step 3: Test Voice Input
1. Open the AI Screen (HomeScreen â†’ AI Mode)
2. Grant microphone permission when prompted
3. Press ğŸ¤ button
4. Speak clearly: "Can you tell me about education schemes?"
5. Press â¹ï¸ to stop recording
6. Watch the magic happen! 

Expected flow:
```
ğŸ¤ Recording... Speak now  â† Shows during recording
                          â†“
ğŸ¤ Can you tell me about education schemes?  â† Your transcribed voice
                          â†“
â³ Processing...  â† AI is thinking
                          â†“
ğŸ”Š Response from AI...  â† AI speaks back
```

---

## Troubleshooting

### Problem: "Microphone permission not granted"
**Solution**: 
- Android: Go to App Settings â†’ Permissions â†’ Grant Microphone
- iOS: Settings â†’ App â†’ Nalam â†’ Microphone â†’ Allow

### Problem: "Could not process voice: Could not understand audio"
**Solution**:
- Speak more clearly and louder
- Reduce background noise
- Try again - speech recognition isn't 100% accurate

### Problem: "Could not connect to AI service"
**Solution**:
1. Check API server is running: `python api_server.py`
2. Check terminal has no errors
3. Verify API_SERVER = 'http://localhost:5000' in AIInteractiveScreen.js
4. On Android emulator: Use `10.0.2.2` instead of `localhost`

### Problem: Text gets cut off in input box
**Solution**: Already handled - input box has `maxLength={500}` and expands up to 100px

### Problem: No audio playing after response
**Solution**:
1. Check volume isn't muted
2. Check gTTS is installed: `pip install gtts`
3. Check `/api/synthesize-audio` endpoint responds correctly
4. Try restarting the app

---

## Testing Checklist

### Voice Recording Test
- [ ] Press ğŸ¤ button
- [ ] See "ğŸ¤ Recording... Speak now"
- [ ] Speak test phrase: "Hello world"
- [ ] Press â¹ï¸ button
- [ ] Recording stops and processes

### Transcription Test
- [ ] After recording, see message with ğŸ¤ emoji
- [ ] Text should match what you said
- [ ] If accuracy is bad, retry with clearer speech

### AI Response Test
- [ ] See AI response appear in chat
- [ ] Response is relevant to your question
- [ ] Response language matches your language preference

### Audio Playback Test
- [ ] See ğŸ”Š badge during audio playing
- [ ] Hear AI response in the correct language
- [ ] Audio quality is natural (gTTS is Google-quality)

### Language Switching Test
- [ ] Press English (ğŸ‡¬ğŸ‡§) button
- [ ] Record: "Hello, I need help"
- [ ] AI responds in English âœ…
- [ ] Press Hindi (ğŸ‡®ğŸ‡³) button
- [ ] Record: "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤®à¥à¤à¥‡ à¤®à¤¦à¤¦ à¤šà¤¾à¤¹à¤¿à¤" (in Hindi)
- [ ] AI responds in Hindi âœ…
- [ ] Press Tamil (ğŸ‡®ğŸ‡³) button  
- [ ] Record: "à®µà®£à®•à¯à®•à®®à¯, à®à®©à®•à¯à®•à¯ à®‰à®¤à®µà®¿ à®¤à¯‡à®µà¯ˆ" (in Tamil)
- [ ] AI responds in Tamil âœ…

### Database Integration Test
- [ ] Press English button
- [ ] Ask: "What schemes am I eligible for?"
- [ ] See response with ğŸ”§ Database query tag
- [ ] AI responds with personalized schemes

### Error Handling Test
- [ ] Click ğŸ¤ and immediately â¹ï¸ (silent audio)
- [ ] Should see: "âŒ Could not process voice: Could not understand audio"
- [ ] Type and send message instead âœ…

---

## Architecture at a Glance

```
Your Voice (Microphone)
        â†“
expo-av Audio.Recording (Records WAV)
        â†“
/api/transcribe (speech_recognition)
        â†“
âœï¸ Message in chat + Transcribed text
        â†“
/api/chat (Gemini Decision Model + Database Tools + Response Model)
        â†“
ğŸ”§ [Optional] Database query if needed
        â†“
/api/synthesize-audio (gTTS MP3 generation)
        â†“
expo-av Audio.Sound (Plays response)
        â†“
ğŸ”Š AI Voice Response
```

---

## Key Files Modified

| File | Change | Status |
|------|--------|--------|
| `client/screens/AIInteractiveScreen.js` | Added voice recording, transcription, and audio playback | âœ… Complete |
| `api_server.py` | Already has `/api/transcribe` endpoint | âœ… Ready |
| `nalam.py` | Reference implementation (unchanged) | âœ… Working |
| `client/package.json` | expo-av already installed | âœ… Ready |

---

## Performance Notes

- **Recording**: Real-time, no latency
- **Transcription**: ~1-3 seconds (network dependent)
- **AI Processing**: ~2-5 seconds (Gemini API latency)
- **Audio Synthesis**: ~1-2 seconds (gTTS generation)
- **Total E2E**: ~5-10 seconds from voice input to voice output

---

## Next Steps

1. **Test Now** (5 minutes)
   - Start API server
   - Start Expo app
   - Record a message and verify it works

2. **Customize** (optional)
   - Change welcome message in `playWelcomeAudio()`
   - Adjust language preference buttons
   - Modify styling in `styles` object

3. **Deploy** (production)
   - Build APK: `eas build --platform android`
   - Build IPA: `eas build --platform ios`
   - Upload to Play Store / App Store

---

**Need help?** Check the VOICE_INTEGRATION_SUMMARY.md for detailed architecture
